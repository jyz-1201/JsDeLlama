import traceback
import warnings

import pandas as pd

warnings.simplefilter('ignore', FutureWarning)

import uuid
from collections import defaultdict
from codebleu import calc_codebleu
from tqdm import tqdm
import code_bert_score
import subprocess
import esprima
import logging
import json
import re
import os

from codes.evaluators.eval_code_with_docker import create_apptainer_container, restart_container, stop_apptainer_container, compile_and_run_JS_code_in_docker
from codes.build_dataset.data_io import TEST_CMD_TAG, read_solution, save_solution, read_jsonl_as_df


def find_whole_word(word, text):
    pattern = r'\b{}\b'.format(re.escape(word))
    match = re.search(pattern, text)
    if match:
        return match.group()
    else:
        return None


class Evaluator:
    def evaluate(self, data: dict) -> dict:
        '''
        @para data: dict, contain all information needed to evaluate a deofuscated result,
            field: 
                task_id: int, unique id of the data
                task_type: str, type of the data
                original: str, original code of the data
                obfuscated: str, obfuscated code of the data
                default_instruction: str, default instruction of the data
                deobfuscated: str, deobfuscated code of the data, generated by a deobfuscator
                language: str, language of the data, [JavaScript]
                test_case[optional]: list[list[str, str]], inputs of the program, in each item, item[0] is input, item[1] is expected output
        @return: dict, evaluation metrics, e.g., {"metric_a": 0.5, "metric_b": 0.6}
        '''
        raise NotImplementedError

class ComplexityEvaluator(Evaluator):
    def calc_metrics_with_escomplex(self, code: str) -> dict:
        run_process = subprocess.run("escomplex --json", input=code.encode(), shell=True, capture_output=True)
        assert run_process.returncode == 0 and run_process.stderr.decode()=="", f"[!] Failed to run escomplex,\n{run_process.stderr.decode()}"
        metrics = json.loads(run_process.stdout.decode())
        return {
            "physical_loc": metrics['aggregate']['sloc']['physical'],
            "logical_loc": metrics['aggregate']['sloc']['logical'],
            "func_mean_loc": metrics['loc'],
            "func_mean_halstead": metrics['effort'],
            "func_mean_cyclomatic": metrics['cyclomatic'],
            "func_num": len(metrics['functions']),
            }

    def evaluate(self, data) -> dict:
        if "code_complexity" in data:
            data.pop("code_complexity")
        obfuscated_code = data.get('obfuscated')
        deobfuscated_code = data.get('deobfuscated')
        original_code = data.get('code') if 'code' in data else data.get('original')
        ori_metrics = self.calc_metrics_with_escomplex(original_code)
        obf_metrics = self.calc_metrics_with_escomplex(obfuscated_code)
        deobf_metrics = self.calc_metrics_with_escomplex(deobfuscated_code)
        difference_score = abs(1 - deobf_metrics['logical_loc'] / ori_metrics['logical_loc'])
        simplification_score = 1 - (deobf_metrics['logical_loc'] / obf_metrics['logical_loc'])
        decrease_cyclomatic = 1 - deobf_metrics['func_mean_cyclomatic']/obf_metrics['func_mean_cyclomatic']
        decrease_halstead = 1 - deobf_metrics['func_mean_halstead']/obf_metrics['func_mean_halstead']
        result = {
            "difference_score": difference_score,
            "simplification_score": simplification_score,
            "decrease_cyclomatic": decrease_cyclomatic,
            "decrease_halstead": decrease_halstead,
        }
        data['code_complexity'] = result
        return result

class CodeBLEUEvaluator(Evaluator):
    def evaluate(self, data) -> dict:
        if "code_bleu" in data:
            data.pop("code_bleu")
        reference = data.get('code') if 'code' in data else data.get('original')
        prediction = data.get('deobfuscated')
        language = data.get('language')
        language = "JavaScript"
        assert language.lower() == "javascript", "[!] Only support JavaScript for now"
        language = {
            "C":"c", 
            "C++":"cpp", 
            "Python":"python",
            "JavaScript":"javascript",}[language] # algin with codebleu
        result = calc_codebleu(references=[reference], predictions=[prediction], 
                               lang=language, weights=(0.25, 0.25, 0.25, 0.25))
        data['code_bleu'] = result
        return result
        # {
        #     "codebleu": code_bleu_score,
        #     "ngram_match_score": ngram_match_score,
        #     "weighted_ngram_match_score": weighted_ngram_match_score,
        #     "syntax_match_score": syntax_match_score,
        #     "dataflow_match_score": dataflow_match_score,
        # }


class CodeBertScoreEvaluator(Evaluator):
    def __init__(self, device=None, batch_size=64) -> None:
        '''
        @device: str, device to run the model, e.g., "cuda:0", "cpu", None for auto
        @per_device_train_batch_size: int, batch size to run the similarity model
        '''
        super().__init__()
        self.device = device
        self.batch_size = batch_size

    def evaluate(self, data) -> dict:
        if "code_bert_score" in data:
            data.pop("code_bert_score")
        reference = data.get('code') if 'code' in data else data.get('original')
        prediction = data.get('deobfuscated')
        language = data.get('language')
        language = "JavaScript"
        assert language.lower() in ["javascript","js"], "[!] Only support JavaScript for now"
        result = code_bert_score.score(cands=[prediction], refs=[reference], lang=language)
        result = [float(i) for i in result] # tensor to float
        #result = {"precision":result[0],"recall":result[1],"F1":result[2],"F3":result[3]}
        result = {"precision":result[0],"recall":result[1],"F1":result[2]}
        data['code_bert_score'] = result
        return result
    
    def evaluate_dataset(self, data_list) -> dict:
        for data in data_list:
            if "code_bert_score" in data:
                data.pop("code_bert_score")
        references = [data.get('code') if 'code' in data else data.get('original') 
                      for data in data_list]
        predictions = [data.get('deobfuscated') for data in data_list]
        language = data_list[0].get('language')
        language = "JavaScript"
        assert language.lower() in ["javascript","js"], "[!] Only support JavaScript for now"
        result = code_bert_score.score(cands=predictions, refs=references, lang=language, device=self.device)
        # print(result)
        result = [i.tolist() for i in result] # tensor to float
        for data, p, r, f in zip(data_list, result[0], result[1], result[2]):
            data['code_bert_score'] = {
                "precision": p,
                "recall": r,
                "F1": f,
                #"F3": f3,
            }
        result = {
            "precision": sum(result[0]) / len(result[0]), 
            "recall": sum(result[1]) / len(result[1]), 
            "F1": sum(result[2]) / len(result[2]), 
            #"F3": sum(result[3]) / len(result[3]), 
            }
        return result
    
class SyntaxEvaluator(Evaluator):
    def is_valid_js(self, code: str) -> bool:
        if code is None or code.strip() == "":
            return False
        run_process = subprocess.run("escomplex --json", input=code.encode(), shell=True, capture_output=True)
        if run_process.returncode == 0 and run_process.stderr.decode()=="":
            return True
        else:
            return False

    def evaluate(self, data) -> dict:
        if "syntax_pass" in data:
            data.pop("syntax_pass")
        prediction: str = data.get('deobfuscated')
        if prediction is None or prediction.strip() == "":
            result = 0
        else:
            try:
                run_process = subprocess.run("escomplex --json", input=prediction.encode(), shell=True, capture_output=True)
                if run_process.returncode == 0 and run_process.stderr.decode()=="":
                    result = 1
                else:
                    result = 0
            except esprima.Error:
                result = 0
        data['syntax_pass'] = result
        return {'pass': result}


class SafeCodeEvaluator(Evaluator):
    """use docker"""
    RESTART_CONTAINER_INTERVAL = 100
    def __init__(self, 
                 contrainer_name="eval_js_container", 
                 apptainer_image="docker://node:latest",
                 default_timeout=2,
                 max_retry=7
                 ) -> None:
        super().__init__()
        retry_cnt = success = 0
        while not success and retry_cnt < max_retry:
            try:
                self.container_name = create_apptainer_container(
                    container_name=f"{contrainer_name}_{uuid.uuid4()}", apptainer_image=apptainer_image)
                success = 1
            except subprocess.CalledProcessError as e:
                print(type(e), e)
                retry_cnt += 1

        self.apptainer_image = apptainer_image
        self.default_timeout = default_timeout
        self.exe_js_cnt = 0 # How many times to execute JS code before restarting the container

    def maintain_container(self):
        if self.exe_js_cnt >= self.RESTART_CONTAINER_INTERVAL:
            logging.info(f"[-] Restart container '{self.container_name}' to avoid resource leak")
            assert restart_container(self.container_name), "[!] Failed to restart container"
            self.exe_js_cnt = 0

    def execute_npm_test(self, data):
        raise NotImplementedError
        test_case = data.get('test_case') if 'test_case' in data else data.get('test_cases')
        test_path = data.get('test_path')
        if test_case.startswith(TEST_CMD_TAG):
            test_cmd = test_case.replace(TEST_CMD_TAG, "")
        else:
            raise NotImplementedError
        # run test
        try:
            run_process = subprocess.run(test_cmd, cwd=test_path, shell=True, capture_output=True, timeout=5)

            if run_process.returncode == 0:
                return 1
            else:
                logging.debug(f"[!] Runtime Error: {run_process.stderr}")
                return 0
        except subprocess.TimeoutExpired:
            logging.debug("[-] Error: Program timeout")
            return 0

    def execute_node_test(self, data):
        self.maintain_container()

        test_case = data.get('test_case') if 'test_case' in data else data.get('test_cases')
        code = data.get('deobfuscated')
        timeout = data.get('timeout', self.default_timeout)
        program_inputs = [t[0] for t in test_case]
        expected_outputs = [t[1] for t in test_case]

        res = compile_and_run_JS_code_in_docker(self.container_name, code, program_inputs, expected_outputs, timeout)
        self.exe_js_cnt += 1
        return res

    def evaluate(self, data) -> dict:
        try:
            test_case = data.get('test_case') if 'test_case' in data else data.get('test_cases')
            if type(test_case) == str:
                res = self.execute_npm_test(data)
            elif type(test_case) == list:
                res = self.execute_node_test(data)
            else:
                raise NotImplementedError
            data['exe_pass'] = res
            return {'pass': res}
        except Exception as e:
            return {'pass': 0}
    
    def stop_container(self):
        stop_apptainer_container(self.container_name)
    
def evaluate_deobfuscation(prediction_file: str, 
                           save_with_metrics: bool = True,
                           contrainer_name: str = "eval_js_container",
                           generated_summary_file: str = "",
                           timeout: float = 5.0):
    logging.info("[+] Start evaluation...")

    if generated_summary_file == "":
        dataset = read_solution(prediction_file)
    else:
        df_prediction = read_jsonl_as_df(prediction_file)
        df_summary = read_jsonl_as_df(generated_summary_file)
        df_subset = df_summary[["filename", "obfuscation_type", "task_type", "generated_summary"]]

        df_merged = pd.merge(df_prediction, df_subset, on=["filename", "obfuscation_type", "task_type"])
        dataset = df_merged.to_dict('records')

    def find_codeblock(raw_output):
        code_out = raw_output
        if '```javascript' in raw_output:
            parts = raw_output.split('```javascript')
            if len(parts) >= 2:
                # first element of `parts` array is a section of text preceding any codeblock.
                # second element is codeblock of LLM-generated code.
                code_out = parts[1]
            if '```' in code_out:
                code_out = code_out.split('```')[0]
        return code_out

    # Apply method find_codeblock to 'deobfuscated' in each dictionary
    dataset = [{**d, 'deobfuscated': find_codeblock(d['deobfuscated'])} for d in dataset]

    # create evaluators
    syntax_evaluator = SyntaxEvaluator()
    complexity_evaluator = ComplexityEvaluator()
    safe_code_evaluator = SafeCodeEvaluator(contrainer_name=contrainer_name)
    code_bleu_evaluator = CodeBLEUEvaluator()
    code_bert_score_evaluator = CodeBertScoreEvaluator(device="cuda:0")
    
    # evaluate
    metrics = defaultdict(float)
    syntax_pass_cnt = 0

    # eval syntax
    for data in tqdm(dataset, desc="syntax evaluation"):
        res_syntax = syntax_evaluator.evaluate(data)
        syntax_pass_cnt += res_syntax['pass']
    logging.warning(f"[+] Syntax pass rate: {syntax_pass_cnt}/{len(dataset)} = {syntax_pass_cnt / len(dataset)}")
    
    # !!!The following metrics are only calculated for the code that passes the syntax check!!!

    # eval codebertscore, use batch speed up
    logging.info(f'[+] Start codebertscore evaluation, using device={code_bert_score_evaluator.device}')
    metrics_cbs = defaultdict(float)

    if syntax_pass_cnt != 0:
        temp_dataset = [d for d in dataset if d['syntax_pass']]
        res_cbs = code_bert_score_evaluator.evaluate_dataset(temp_dataset)
        temp_dataset = {d['task_id']:d['code_bert_score'] for d in temp_dataset}
        for d in dataset:
            if d['task_id'] in temp_dataset:
                d['code_bert_score'] = temp_dataset[d['task_id']]
        for submetric, v in res_cbs.items():
            metrics_cbs[f'code_bert_score_{submetric}'] = v
        logging.info(json.dumps(metrics_cbs, indent=4))
    else:
        metrics_cbs['code_bert_score_precision'] = 0
        metrics_cbs['code_bert_score_recall'] = 0
        metrics_cbs['code_bert_score_F1 '] = 0

    logging.info('[+] Start other evaluation...')
    for data in tqdm(dataset, desc="evaluation"):
        if data['syntax_pass']==0: # jump other metrics if syntax check not pass
            continue
        print(data)
        # print("hii1")
        res_exe = safe_code_evaluator.evaluate(data)
        metrics['exe_pass'] += res_exe['pass']
        # print("hii2")
        res_bleu = code_bleu_evaluator.evaluate(data)
        for submetric, v in res_bleu.items():
            metrics[f'code_bleu_{submetric}'] += v
        # print("hii3")
        res_cpx = complexity_evaluator.evaluate(data)
        for submetric, v in res_cpx.items():
            metrics[f'code_complexity_{submetric}'] += v
        # print("hii4")

    safe_code_evaluator.stop_container()

    for k in metrics:
        metrics[k] = metrics[k]/syntax_pass_cnt
    
    # add syntax pass rate
    metrics['syntax_pass'] = syntax_pass_cnt / len(dataset)

    # add codebertscore metrics
    metrics.update(metrics_cbs)
    
    # move syntax_pass to the first
    syntax_pass = metrics.pop('syntax_pass')
    metrics = {'syntax_pass':syntax_pass, **metrics}
    
    # log 
    print(json.dumps(metrics, indent=4))

    # save prediction with metrics
    if save_with_metrics:
        fp, ext = os.path.splitext(prediction_file)
        save_to = fp+'.metrics'+ext
        logging.info(f"[+] Save metrics into file: {save_to}")
        save_solution(dataset, save_to)
    
    return metrics
